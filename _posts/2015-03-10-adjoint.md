---
layout: post
title: Adjoint
disqus: y
---

The adjoint solver is nothing but the transpose of the solver. Suppose we have a equation as

$$P(\alpha) u - f = 0$$

where $P(\alpha)$ is a differential operator depending on $\alpha$. Then it is easy to invert it to solve our solution $u = u(\alpha, f)$.

However, what if we want something else, like a functional of our interest $J(u)$. And usually this is a functional which we want to minimize, then common optimization methods will need the gradient.

$$\frac{\partial J(u)}{\partial \alpha} = \frac{\partial J(u)}{\partial u}\frac{\partial u}{\partial \alpha}$$

where $\frac{\partial u}{\partial \alpha}$ can be deducted from the explicit solution (if we have), otherwise, we are getting another equation:

$$\frac{\partial P(\alpha) u}{\partial \alpha} = P\frac{\partial u}{\partial \alpha} + \frac{\partial P}{\partial \alpha} u = 0$$

which means

$$\frac{\partial J}{\partial \alpha} = -\frac{\partial J}{\partial u} P^{-1} \frac{\partial P}{\partial \alpha} u$$

transpose it,

$$\frac{\partial J}{\partial \alpha}^{\ast} = -u^{\ast}\frac{\partial P}{\partial \alpha} \underline{P^{-\ast} \frac{\partial J}{\partial u}^{\ast}}$$


We do this is because $\frac{\partial J}{\partial u}$ is vector, can be easily solved through adjoint solver $P^{-\ast}$.


Another problem is the derivative of $P$, i.e. $\frac{\partial P}{\partial \alpha}$. If $P$ is simply a $n \times{n}$ matrix, and there is $m$ dofs of $\alpha$, then $\frac{\partial P}{\partial \alpha}$ will be a $m\times 1$ vector of matrix of $n\times{n}$. However, it is not a problem with numerical.

- solve the forward problem and get $u$, with $O(n^3)$
- solve the adjoint problem and get $\lambda =\underline{P^{-\ast} \frac{\partial J}{\partial u}^{\ast}}$, with $O(n^3)$
- calculate the matrix multiplication. Roughly $O(n^2 m)$.

It will be much more faster if $P$ is sparse.
