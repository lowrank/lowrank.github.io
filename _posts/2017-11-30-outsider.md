---
layout: post
title: 始终是门外汉
disqus: y
---

本来说好的不沾任何所谓的``Data Science``，但是坑始终是坑，不是坑自己就是坑别人。

以前不知道在哪里看的，在良序的鄙视链里，搞数学的理所应当地就该鄙视搞数据科学的。于是去知乎上看来看，arxiv上看了看，又看了看一本叫``deep learning``的书。我沉默了。

每一篇论文，结构都差不多，大致是提出一个所谓的model，然后在几个测试数据上跑一跑，拿来和别人比一比。内容上并无多少数学，就连公式也寥寥无几，只是在某些数据上可以跑出好结果，并不代表就同类的数据就可以跑出好结果，可是这些论文的引用都出乎意料的高。

我想了想，大概是因为我是一个门外汉，没搞懂他们在说什么吧。

那我就自说自话吧。

首先，所谓的数据科学就没有在一个合适的角度看待问题。并不是所有的数据都有意义，数据里的噪音也未见得就要服从正态分布，再说数据也并非是``unbiased``。所有的问题都限制了这些模型的应用范围。

就算在一个合适的应用范围内，是否算法和模型就一定可以解决实际问题。一个一个地试显然是最愚蠢的办法，而调参数就正好是这种。

就算是算法可以在某一个参数设定下得到理想的结果，是不是就一定是最优了。到底什么样的数据可以达到最优或者是近似最优。还是说这个问题本身并不可证：我可以找出对应的例子，却无法找出所有的例子。

为什么图片，语言可以被处理，而很多看似简单的方程不能被目前的机器学习所解决。我粗浅的理解是：它们的区别是秩。 数学上来说，我们是需要内蕴的结构来描述数据的复杂度，也就是能够被表示到什么程度。秩 (rank)，直接告诉我们这样的一个内蕴信息。

比方说，最简单的矩阵或者是张量，它的秩就是基的个数。然而，我们通常并不需要完全准确的矩阵，只要矩阵乘法足够准确就足够了。如果我们试着去QR分解一个矩阵，把R里的对角元素排序，扔掉比较小的行， 这样的矩阵就足够逼近之前的矩阵了。这里的秩，是和threshold $\epsilon$ 相关的。它也立刻给出了一个关于秩的定义。

$$\text{rank}(A, \epsilon) = \arg \min_m \|A - \sum_{k=1}^m p_k q^{T}_k\|\le \epsilon \|A\|$$

这里 $p_k,q_k$ 都是rank-1的元素。对于线性的问题，QR分解或者是SVD都可以得到对应的解。当$A$并不是一个线性的算子时，我们需要重新思考下$A$的含义。

如果说 $A$ 里的每一列是每个输入元素对应到的像的值。那么线性的情况就是线性回归。非线性的时候，我们需要定义秩的含义。

$$\text{rank}(A,\epsilon) = \arg\min_m \|A(x,y) - \sum_{k=1}^m p_k(x)q_k(y)\|\le \epsilon \|A\|$$

这里有几个问题。

1. 取什么样的度量
2. 怎么取这些basis
3. $m$和$\epsilon$的关系是什么

如果说我们考虑的模型是有限维度的，那么这里的度量确切地说是防止退化的维度。我想唯一的办法就是normalization，这基于对数据的充分信任。我们会有那么一点点的概率被这份信任辜负了，当然，小概率事件必然发生。解决的办法就是要有足够多的好数据，尽量少的坏点。

这些basis事实上没有任何要求，最简单的就是Fourier basis或者wavelet，具备对信息描述的能力，插值也有一定的置信度。说起插值，由于很多情况下数据的label是离散的，所以插值并不容易。这也是在高维像空间里插值的难点，太奇怪的采样会导致Lebesgue数并不那么mild。于是我们还是得对basis有一点要求才行，这也限制了描述函数的能力。

取basis的过程也就是逐步确定$m$的过程，如果用分层结构，我们其实是在赌，赌它的秩足够低，低到 $(\log\epsilon)^n$ 这个量级。碰巧地是， 图片大多数时候都满足这个要求，jpeg格式也就是在做这件事。压缩图片大致也是这个道理。不过，如果信息含有大量的高频信号和噪声，那么我们可以确定无法简单地压缩，或者秩不会低的。

基本所有的``DL``都落入了这个坑里，自从``resnet``抄了几十年前的矩阵分层分解的想法之后（这里的代表作是multigrid），简直是把搞数据科学的人刺激疯了一样，不论是怎么样的细节处理，都是在尽可能地减少秩或者是在稳定局部的Lebesgue常数或是在添加各种regularization，在中间找一个平衡。

作为一个门外汉，不知不觉喷了好多连我自己都不懂的话。

惭愧惭愧。