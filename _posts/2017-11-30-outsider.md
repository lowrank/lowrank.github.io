---
layout: post
title: 始终是门外汉
disqus: y
---

本来说好的不沾任何所谓的``Data Science``，但是坑始终是坑，不是坑自己就是坑别人。

以前不知道在哪里看的，在良序的鄙视链里，搞数学的理所应当地就该鄙视搞数据科学的。于是去知乎上看来看，arxiv上看了看，又看了看一本叫``deep learning``的书。我沉默了。

每一篇论文，结构都差不多，大致是提出一个所谓的model，然后在几个测试数据上跑一跑，拿来和别人比一比。内容上并无多少数学，就连公式也寥寥无几，只是在某些数据上可以work，并不代表就一定work，可是citation都出乎意料的高。

我想了想，大概是因为我是一个门外汉，没搞懂他们在说什么吧。

那我就自说自话吧。

我所理解的``DL``，大概是基于许多看似合理的假设的。比方说数据的sanity，也就是说，数据对应关系是一个足够低维的映射，大多数的数据是多余的。数学里面有很多类似的东西，intrinsic的维度足够低，sampling不再是个问题。就好比解一个线性的$d$维椭圆的PDE，你实际上不需要把解sample在很多点上，因为它的Green's function的rank比较低。或者说存在函数 $p_k(x),q_k(y)$, $m=O(\log(\epsilon)^{d+1})$ 使得

$$\large\|G(x, y) - \sum_{k=1}^m p_k(x) q_k(y)\|<\epsilon\|G\|$$

所谓的学习，就是是一个找Green's function的过程。假设全部数据就是一个映射 $f:X\to Y$, 尽管我们只能sample一部分, 但如果$f$相对来说有比较低的rank，类似地我们可以先近似地找一个$G_0$ 使得


$$\large G_0= \arg\min_{G} \|f(z) - \int_{Z} G(\omega,z)g(\omega)d\omega\|_Y + \beta \|G\|$$

这里 $Z$ 是sampling空间, $g$是某一个normalization。然后迭代地把剩下的residual按照同样的办法给处理了。于是得到了一个序列$G_k$. 基本上就是在做tree逼近。当然这里 $G_k$ 的选择太多， 需要一些限制才可以。

``DL``这里的区别就是一次性地把所有的 $G_k$ 给找出来，避免了最开始的误差propagation，另一个区别就是积分变成了某个固定的非线性函数 (e.g. ``Relu``) 乘以$\delta(\omega - z)$。我们也可以考虑别的 $G$, 比方说 $G(\omega, z)\propto \|\omega - z\|^{-\alpha}$，至于好还是不好，真的是依赖数据本身。

如果说这种分层的分解 (Hierachical) 的误差按照层数递减的速度足够快， 我们有理由相信这样的数据是近似low rank的。就好比 ``resnet`` 虽然看起来层数惊人，intrinsic的层数不可能太多， 原因就是数据本身的rank不够大。

所以，我们可以很容易地造出数据让任何``DL``都不能轻松地逼近，即使数据足够光滑也足够低维。

感觉好悲观啊，大概是因为我是一个门外汉，没搞懂他们在说什么吧。